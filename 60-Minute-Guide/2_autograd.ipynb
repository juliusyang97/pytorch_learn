{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bea78130",
   "metadata": {},
   "source": [
    "# AutoGrad : 自动求导  \n",
    "torch.autograd是pytorch自动求导的工具，也是所有神经网络的核心。我们首先先简单了解一下这个包如何训练神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9390f58a",
   "metadata": {},
   "source": [
    "## 背景介绍  \n",
    "\n",
    "神经网络(NNs)是作用在输入数据上的一系列嵌套函数的集合，这些函数由权重和误差来定义，被存储在PyTorch中的tensors中。\n",
    "\n",
    "神经网络训练的两个步骤：\n",
    "\n",
    "**前向传播：** 在前向传播中，神经网络通过将接收到的数据与每一层对应的权重和误差进行运算来对正确的输出做出最好的预测。\n",
    "\n",
    "**反向传播：** 在反向传播中，神经网络调整其参数使得其与输出误差成比例。反向传播基于梯度下降策略，是链式求导法则的一个应用，以目标的负梯度方向对参数进行调整。\n",
    "\n",
    "更加详细的介绍可以参照下述地址：[video from 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c8dc1d",
   "metadata": {},
   "source": [
    "## 在Pytorch 中的应用  \n",
    "\n",
    "来看一个简单的示例，我们从torchvision加载一个预先训练好的resnet18模型，接着创建一个随机数据tensor来表示一有3个通道、高度和宽度为64的图像，其对应的标签初始化为一些随机值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeacae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c84e3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "model = torchvision.models.resnet18(pretrained=True)\n",
    "data = torch.rand(1, 3, 64, 64)\n",
    "labels = torch.rand(1, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee168f",
   "metadata": {},
   "source": [
    "接下来，我们将输入数据向输出方向传播到模型的每一层中来预测输出，这就是**前向传播** 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d44cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model(data) # 前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21ee1c",
   "metadata": {},
   "source": [
    "我们利用模型的预测和对应的标签来计算误差（loss），然后反向传播误差。完成计算后，您可以调用`.backward()`并自动计算所有梯度。此张量的梯度将累积到`.grad`属性中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "700f81ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = (prediction - labels).sum()\n",
    "loss.backward() # 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c63f55e",
   "metadata": {},
   "source": [
    "接着，我们加载一个优化器，在本例中，SGD的学习率为0.01，momentum 为0.9。我们在优化器中注册模型的所有参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d49947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr= 0.001, momentum= 0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d431e68",
   "metadata": {},
   "source": [
    "最后，我们调用`.step()`来执行梯度下降，优化器通过存储在`.grad`中的梯度来调整每个参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9324ae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() # 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e86a697",
   "metadata": {},
   "source": [
    "现在，你已经具备了训练神经网络所需所有条件。下面几节详细介绍了Autograd包的工作原理——可以跳过它们。\n",
    "\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bf953a",
   "metadata": {},
   "source": [
    "## Autograd中的求导  \n",
    "\n",
    "先来看一下autograd是如何收集梯度的。我们创建两个张量a和b并设置requires_grad = True以跟踪它的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2e1ed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "672f92a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([2., 3.], requires_grad= True)\n",
    "b = torch.tensor([6., 4.], requires_grad= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17aceea4",
   "metadata": {},
   "source": [
    "接着在a和b的基础上创建张量Q\n",
    "\n",
    "\\begin{align}Q = 3a^3 - b^2\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a913c135",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870fe269",
   "metadata": {},
   "source": [
    "假设a和b是一个神经网络的权重，Q是它的误差，在神经网络训练中，我们需要w.r.t参数的误差梯度，即  \n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial a} = 9a^2\\end{align}\n",
    "\n",
    "\\begin{align}\\frac{\\partial Q}{\\partial b} = -2b\\end{align}\n",
    "\n",
    "当我们调用Q的.backward()时，autograd计算这些梯度并把它们存储在张量的 .grad属性中。\n",
    "\n",
    "我们需要在Q.backward()中显式传递gradient，gradient是一个与Q相同形状的张量，它表示Q w.r.t本身的梯度，即  \n",
    "\n",
    "\\begin{align}\\frac{dQ}{dQ} = 1\\end{align}\n",
    "\n",
    "同样，我们也可以将Q聚合为一个标量并隐式向后调用，如Q.sum().backward()。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75408a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "external_grad = torch.tensor([1., 1,])\n",
    "Q.backward(gradient=external_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c71f727",
   "metadata": {},
   "source": [
    "现在梯度都被存放在a.grad和b.grad中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5a81ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True])\n",
      "tensor([True, True])\n"
     ]
    }
   ],
   "source": [
    "# 检查一下存储的梯度都正确\n",
    "print(9*a**2 == a.grad)\n",
    "print(-2*b == b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f1d75",
   "metadata": {},
   "source": [
    "## 选读----用autograd进行向量计算  \n",
    "\n",
    "在数学上，如果你有一个向量值函数𝑦⃗ =𝑓(𝑥⃗ ) ，则𝑦⃗ 相对于𝑥⃗ 的梯度是雅可比矩阵：  \n",
    "\n",
    "\\begin{align}J\n",
    "     =\n",
    "      \\left(\\begin{array}{cc}\n",
    "      \\frac{\\partial \\bf{y}}{\\partial x_{1}} &\n",
    "      ... &\n",
    "      \\frac{\\partial \\bf{y}}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\n",
    "     =\n",
    "     \\left(\\begin{array}{ccc}\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{1}}{\\partial x_{n}}\\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial y_{m}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\\end{align}\n",
    "      \n",
    "\n",
    "一般来说，torch.autograd是一个计算雅可比向量积的引擎。 也就是说，给定任何向量𝑣=(𝑣1𝑣2...𝑣𝑚)𝑇，计算乘积$J^{T}\\cdot \\vec{v}$。  \n",
    "\n",
    "如果𝑣恰好是标量函数的梯度𝑙=𝑔(𝑦⃗ )，即\n",
    " \n",
    "\\begin{align}\\vec{v}\n",
    "   =\n",
    "   \\left(\\begin{array}{ccc}\\frac{\\partial l}{\\partial y_{1}} & \\cdots & \\frac{\\partial l}{\\partial y_{m}}\\end{array}\\right)^{T}\\end{align}\n",
    " \n",
    " 然后根据链式法则，雅可比向量乘积将是𝑙相对于𝑥⃗ 的梯度  \n",
    " \n",
    "\\begin{align}J^{T}\\cdot \\vec{v}=\\left(\\begin{array}{ccc}\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{1}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{1}}\\\\\n",
    "      \\vdots & \\ddots & \\vdots\\\\\n",
    "      \\frac{\\partial y_{1}}{\\partial x_{n}} & \\cdots & \\frac{\\partial y_{m}}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\\left(\\begin{array}{c}\n",
    "      \\frac{\\partial l}{\\partial y_{1}}\\\\\n",
    "      \\vdots\\\\\n",
    "      \\frac{\\partial l}{\\partial y_{m}}\n",
    "      \\end{array}\\right)=\\left(\\begin{array}{c}\n",
    "      \\frac{\\partial l}{\\partial x_{1}}\\\\\n",
    "      \\vdots\\\\\n",
    "      \\frac{\\partial l}{\\partial x_{n}}\n",
    "      \\end{array}\\right)\\end{align}\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "雅可比向量积的这种特性使得将外部梯度馈送到具有非标量输出的模型中非常方便。external_grad 代表$\\vec{v}$\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beed98c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
